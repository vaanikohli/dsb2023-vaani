---
title: 'Homework 3: Databases, web scraping, and a basic Shiny app'
author: "VAANI KOHLI"
date: "25/05/2023"
output:
  word_document:
    toc: yes
  pdf_document:
    toc: yes
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(wbstats)
library(tictoc)
library(skimr)
library(countrycode)
library(here)
library(DBI)
library(dbplyr)
library(arrow)
library(rvest)
library(robotstxt) # check if we're allowed to scrape the data
library(scales)
library(sf)
library(readxl)
```

# Money in UK politics

[The Westminster Accounts](https://news.sky.com/story/the-westminster-accounts-12786091), a recent collaboration between Sky News and Tortoise Media, examines the flow of money through UK politics. It does so by combining data from three key sources:

1.  [Register of Members' Financial Interests](https://www.parliament.uk/mps-lords-and-offices/standards-and-financial-interests/parliamentary-commissioner-for-standards/registers-of-interests/register-of-members-financial-interests/),
2.  [Electoral Commission records of donations to parties](http://search.electoralcommission.org.uk/English/Search/Donations), and
3.  [Register of All-Party Parliamentary Groups](https://www.parliament.uk/mps-lords-and-offices/standards-and-financial-interests/parliamentary-commissioner-for-standards/registers-of-interests/register-of-all-party-party-parliamentary-groups/).

You can [search and explore the results](https://news.sky.com/story/westminster-accounts-search-for-your-mp-or-enter-your-full-postcode-12771627) through the collaboration's interactive database. Simon Willison [has extracted a database](https://til.simonwillison.net/shot-scraper/scraping-flourish) and this is what we will be working with. If you want to read more about [the project's methodology](https://www.tortoisemedia.com/2023/01/08/the-westminster-accounts-methodology/).

## Open a connection to the database

The database made available by Simon Willison is an `SQLite` database

```{r}
sky_westminster <- DBI::dbConnect(
  drv = RSQLite::SQLite(),
  dbname = here::here("data", "sky-westminster-files.db")
)
```

How many tables does the database have?

```{r}
DBI::dbListTables(sky_westminster)
# The database has 7 tables
```

```{r}
# Which MP has received the most amount of money?
payments <- dplyr::tbl(sky_westminster, "payments")
members <- dplyr::tbl(sky_westminster, "members")

payments %>%
  left_join(members, by = c("member_id" = "id")) %>%
  group_by(name) %>%
  summarise(total = sum(value)) %>% 
  arrange(desc(total)) %>% 
  collect()

# Theresa May with 2.8 million GBP.
```

## Any `entity` that accounts for more than 5% of all donations?

Is there any `entity` whose donations account for more than 5% of the total payments given to MPs over the 2020-2022 interval? Who are they and who did they give money to?

```{r}

payments %>%
  group_by(entity) %>%
  summarise(total = sum(value)) %>%
  arrange(desc(total)) %>%
  collect()

payments %>%
  left_join(members, by = c("member_id" = "id")) %>%
  filter(entity == "Withers LLP") %>%
  group_by(name) %>%
  summarise(total = sum(value)) %>%
  arrange(desc(total))

# Withers LLP accounts for more than 5% of total payments given to MPs over the specified time period. The politician they donated this money to is Sir Geoffrey Cox.
```

## Do `entity` donors give to a single party or not?

-   How many distinct entities who paid money to MPS are there?
-   How many (as a number and %) donated to MPs belonging to a single party only?

```{r}
payments %>%
  summarise(distinct_entities = n_distinct(entity))

#There are 2213 distinct entities that paid money to MPs

party_donations <- tbl(sky_westminster, "party_donations")
parties <- tbl(sky_westminster, "parties")
party_donations %>%
  left_join(parties, by = c("party_id" = "id")) %>%
  distinct(entity, .keep_all = TRUE) %>%
  summarise(count = n())

#There are 1077 entities that donated to a single party only. This is 48.6% of the total distinct entities.
```

## Which party has raised the greatest amount of money in each of the years 2020-2022?

```{r}
#The Conservative party has raised the greatest amount of money in each of the years from 2020-2022.

party_donations <- party_donations %>%
  mutate(date = sql("date(date)")) %>%
  mutate(year = year(date))

party_donations_year <- party_donations %>%
  left_join(parties, by = c("party_id" = "id")) %>%
  group_by(year, name) %>%
  summarise(total = sum(value)) %>%
  mutate(proportion = total/sum(total)) %>%
  collect()

threshold <- 100000
party_donations_year1 <- party_donations_year %>%
  filter(total >= threshold)

party_donations_year1$name <- fct_reorder(party_donations_year1$name, desc(party_donations_year1$total))

ggplot(party_donations_year1, aes(x = factor(year), y = total, fill = name)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Year", y = "Total", title = "Conservatives have captured the majority of political donations", subtitle = "Donations to political parties, 2020-2022") +  
  scale_fill_viridis_d() +
  scale_y_continuous(labels = scales::comma) +  
  theme_minimal()
```

```{r}
dbDisconnect(sky_westminster)
```

# Money in US politics

In the United States, [*"only American citizens (and immigrants with green cards) can contribute to federal politics, but the American divisions of foreign companies can form political action committees (PACs) and collect contributions from their American employees."*](https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs)

We will scrape and work with data foreign connected PACs that donate to US political campaigns. The data for foreign connected PAC contributions in the 2022 election cycle can be found at <https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022>. Then, we will use a similar approach to get data such contributions from previous years so that we can examine trends over time.

All data come from [OpenSecrets.org](https://www.opensecrets.org), a *"website tracking the influence of money on U.S. politics, and how that money affects policy and citizens' lives"*.

```{r}
#| label: allow-scraping-opensecrets
#| warning: false
#| message: false

library(robotstxt)
paths_allowed("https://www.opensecrets.org")

base_url <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"

contributions_tables <- base_url %>%
  read_html()

contributions <- base_url %>%
  read_html() %>%
  html_nodes(css="table") %>% 
  html_table()

contributions <- as.data.frame(contributions)
```

-   Clean the data:

```{r}
# write a function to parse_currency
parse_currency <- function(x){
  x %>%
    
    # remove dollar signs
    str_remove("\\$") %>%
    
    # remove all occurrences of commas
    str_remove_all(",") %>%
    
    # convert to numeric
    as.numeric()
}

# clean country/parent co and contributions 
contributions <- contributions %>%
  janitor::clean_names() %>% 
  separate(country_of_origin_parent_company, 
           into = c("country", "parent"), 
           sep = "/", 
           extra = "merge") %>%
  mutate(
    total = parse_currency(total),
    dems = parse_currency(dems),
    repubs = parse_currency(repubs)
  )
```

-   Write a function called `scrape_pac()` that scrapes information from the Open Secrets webpage for foreign-connected PAC contributions in a given year.

-   Define the URLs for 2022, 2020, and 2000 contributions. Then, test your function using these URLs as inputs. Does the function seem to do what you expected it to do?

-   Map the `scrape_pac()` function over `urls` in a way that will result in a data frame called `contributions_all`.

-   Write the data frame to a csv file called `contributions-all.csv` in the `data` folder.

```{r}
scrape_pac <- function(url) {
  
  year <- str_sub(url, -4)
  
  contributions_tables <- base_url %>%
  read_html()

contributions <- base_url %>%
  read_html() %>%
  html_nodes(css="table") %>% 
  html_table()

contributions <- as.data.frame(contributions)
 
contributions <- contributions %>%
    janitor::clean_names() %>% 
    separate(country_of_origin_parent_company, 
             into = c("country", "parent"), 
             sep = "/", 
             extra = "merge") %>%
    mutate(
      total = parse_currency(total),
      dems = parse_currency(dems),
      repubs = parse_currency(repubs),
      year = as.numeric(year)
      
    )
  
  return(contributions)
}

url_2022 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
url_2020 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020"
url_2000 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000"

contributions_2022 <- scrape_pac(url_2022)

contributions_2020 <- scrape_pac(url_2020)

contributions_2000 <- scrape_pac(url_2000)

years <- seq(2000, 2022, by = 2)

base_url <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/"
urls <- paste0(base_url, years)

contributions_all <- map_df(urls, scrape_pac)

write.csv(contributions_all, file = "data/contributions-all.csv", row.names = FALSE)
```

# Scraping consulting jobs

The website [https://www.consultancy.uk/jobs/](https://www.consultancy.uk/jobs) lists job openings for consulting jobs.

```{r}
#| label: consulting_jobs_url

library(robotstxt)
paths_allowed("https://www.consultancy.uk") #is it ok to scrape?

base_url <- "https://www.consultancy.uk/jobs/page/1"

listings_html <- base_url %>%
  read_html()

```

Identify the CSS selectors in order to extract the relevant information from this page, namely

1.  job - #vacaturenaam
2.  firm - #bedrijf
3.  functional area - #dataTable \> tbody \> tr:nth-child(1) \> th.hide-tablet-and-less
4.  type - #dataTable \> tbody \> tr:nth-child(1) \> th.hide-tablet-landscape

Can you get all pages of ads, and not just the first one, `https://www.consultancy.uk/jobs/page/1` into a dataframe?

```{r}
get_page <- function(pagenumber) {
  
  
  base_url <- "https://www.consultancy.uk/jobs/page/"
  url <- str_c(base_url, pagenumber)
  
  address <- read_html(url)
  
  jobs <- address %>%
    html_nodes(".title") %>% 
    html_text()
  
  
  firm <- address %>%
    html_nodes(".hide-phone .row-link") %>% 
    html_text()
  
  
  functional_area <- address %>%
    html_nodes(".initial") %>% 
    html_text2()
  
  
  type  <- address %>%
    html_nodes(".hide-tablet-landscape .row-link") %>% 
    html_text2()
  
  jobs_df <- tibble(
    jobs = jobs, 
     firm = firm, 
     functional_area = functional_area, 
     type = type,
    page = pagenumber
  )
  
  return(jobs_df)
}


base_url <- "https://www.consultancy.uk/jobs/page/"

pages <-  1:8


alljobs <- map_df(pages, get_page)

write.csv(alljobs, file = "data/all-jobs.csv", row.names = FALSE)
```

# Details

-   Who did you collaborate with: Got a lot of help from Kostis while working through the set
-   Approximately how much time did you spend on this problem set: 8-10 hours
-   What, if anything, gave you the most trouble: Still getting used to the language and syntax and just the basics of writing code. I was not able to complete the set on the CDC data, but will slowly work through it so I can get to the solutions.
